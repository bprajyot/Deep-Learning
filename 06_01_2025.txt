1. History of Machine Learning
2. History of Deep Learning
3. Types of Machine Learning and Deep Learning
4. Different Applications of Machine Learning and Deep Learning
5. Different Libraries used to implement Machine Learning and Deep Learning
6. Math behind libraries
(sources: dataversity, machinelearningmodels)

======================================================================

1. History of ML:
- According to the Arthur Samuel, one of the pioneers in the field of Machine Learning, Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed. His definition highlights the core idea of machine learning, which is, enabling computers to learn from data and improve over time.

- Tom Mitchell, in his definition of ML, emphasizes the importance of experience and performance measurement in the learning process which goes like: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

- 1950 to 1960
	The history of machine learning dates back to the 1950s and 1960s when researchers in artificial intelligence (AI) began exploring ways to enable machines to learn from data. One of the earliest attempts was the creation of programs that could play games like chess and checkers. These early efforts laid the groundwork for more advanced machine learning techniques by demonstrating that machines could be programmed to perform tasks traditionally thought to require human intelligence.
	In the 1950s, Alan Turing, a pioneer in computer science, proposed the concept of a "learning machine" that could modify its behavior based on experience. This idea was a precursor to modern machine learning, emphasizing the potential for machines to improve their performance over time. Turing's work, along with that of other early AI researchers, set the stage for future developments in the field.
	The 1960s saw the development of some of the first machine learning algorithms, including the nearest neighbor algorithm. Researchers began to understand the potential of using statistical methods and probability theory to enable machines to make decisions based on data. This period marked the beginning of a shift from rule-based systems to data-driven approaches in AI research.

- Perceptron Algorithm by Rosenbaltt
	In 1957, Frank Rosenblatt introduced the perceptron algorithm, one of the earliest breakthroughs in machine learning. The perceptron is a type of artificial neuron that serves as a building block for neural networks. Rosenblatt's work demonstrated that machines could learn to recognize patterns and make decisions based on input data, paving the way for future developments in neural network research.
	The perceptron algorithm works by adjusting the weights of input features to minimize the error in classification tasks. This adjustment is done through a process called gradient descent, which iteratively updates the weights based on the error of the predictions. Although the original perceptron was limited to linear decision boundaries, it laid the foundation for more complex neural network architectures that could handle non-linear relationships.
	Rosenblatt's perceptron sparked significant interest in the field of machine learning and led to the development of multilayer perceptrons (MLPs) and backpropagation, techniques that enabled the training of deeper neural networks. These advancements allowed for more accurate and efficient learning from data, further advancing the capabilities of machine learning systems.

- Neural Networks and SVM
	The 1980s and 1990s marked a period of resurgence in machine learning, driven by the introduction of new techniques and algorithms. One of the key developments during this time was the backpropagation algorithm, which enabled the training of deep neural networks. Backpropagation allows for the efficient calculation of gradients, making it possible to train multilayer perceptrons with multiple hidden layers.
	Another significant advancement was the development of support vector machines (SVMs) by Vladimir Vapnik and his colleagues. SVMs are powerful classification algorithms that work by finding the optimal hyperplane that separates data points of different classes. They are particularly effective in high-dimensional spaces and have become a staple in the machine learning toolkit.

- Big Data
	The 2000s witnessed the rise of big data, characterized by the generation and collection of vast amounts of data from various sources such as social media, sensors, and e-commerce platforms. This explosion of data provided a rich resource for training machine learning models, enabling them to learn from diverse and extensive datasets.
	With the availability of big data, machine learning models could be trained to recognize more complex patterns and make more accurate predictions. The increased volume and variety of data also led to the development of new data processing frameworks like Hadoop and Spark, which facilitated the storage and processing of large datasets.
	The rise of big data also highlighted the importance of feature engineering, the process of selecting and transforming input features to improve model performance. Techniques such as one-hot encoding, normalization, and dimensionality reduction became essential for handling large and high-dimensional datasets. The combination of big data and advanced feature engineering significantly enhanced the capabilities of machine learning models.


===========================================================================================================================================================================


History of Deep Learning
- Deep Learning, is a more evolved branch of machine learning, and uses layers of algorithms to process data, and imitate the thinking process, or to develop abstractions.

- According to Geoffrey Hinton, Deep Learning is a way to automatically learn representations from data by using multiple layers of neural networks. It allows computers to understand and interpret complex patterns and structures in data.

- The history of deep learning can be traced back to 1943, when Walter Pitts and Warren McCulloch created a computer model based on the neural networks of the human brain. They used a combination of algorithms and mathematics they called “threshold logic” to mimic the thought process. Since that time, Deep Learning has evolved steadily, with only two significant breaks in its development. Both were tied to the infamous Artificial Intelligence winters.

- 1960
	Henry J. Kelley is given credit for developing the basics of a continuous Back Propagation Model in 1960. In 1962, a simpler version based only on the chain rule was developed by Stuart Dreyfus. While the concept of back propagation (the backward propagation of errors for purposes of training) did exist in the early 1960s, it was clumsy and inefficient, and would not become useful until 1985. 
	The earliest efforts in developing deep learning algorithms came from Alexey Grigoryevich Ivakhnenko and Valentin Grigorʹevich Lapa in 1965. They used models with polynomial activation functions, that were then analyzed statistically. From each layer, the best statistically chosen features were then forwarded on to the next layer which was a slow, manual process.

-1970
	The first convolutional neural networks were used by Kunihiko Fukushima. Fukushima designed neural networks with multiple pooling and convolutional layers. In 1979, he developed an artificial neural network, called Neocognitron, which used a hierarchical, multilayered design. This design allowed the computer the learn to recognize visual patterns. The networks resembled modern versions, but were trained with a reinforcement strategy of recurring activation in multiple layers, which gained strength over time. Additionally, Fukushima’s design allowed important features to be adjusted manually by increasing the weight of certain connections. Many of the concepts of Neocognitron continue to be used.
	The use of top-down connections and new learning methods have allowed for a variety of neural networks to be realized. When more than one pattern is presented at the same time, the Selective Attention Model can separate and recognize individual patterns by shifting its attention from one to the other. A modern Neocognitron can not only identify patterns with missing information, but can also complete the image by adding the missing information. This could be described as inference.
	Back propagation, the use of errors in training deep learning models, evolved significantly in 1970. This was when Seppo Linnainmaa wrote his master’s thesis, including a FORTRAN code for back propagation.
	Unfortunately, the concept was not applied to neural networks until 1985. This was when Rumelhart, Williams, and Hinton demonstrated back propagation in a neural network could provide “interesting” distribution representations. Philosophically, this discovery brought to light the question within cognitive psychology of whether human understanding relies on symbolic logic (computationalism) or distributed representations (connectionism).

-1980s and 1990s
	In 1989, Yann LeCun provided the first practical demonstration of backpropagation at Bell Labs. He combined convolutional neural networks with back propagation onto read handwritten digits. This system was eventually used to read the numbers of handwritten checks.
	This time is also when the second AI winter (1985-90s) kicked in, which also effected research for neural networks and deep learning. Various overly-optimistic individuals had exaggerated the immediate potential of Artificial Intelligence, breaking expectations and angering investors. The anger was so intense, the phrase Artificial Intelligence reached pseudoscience status. Fortunately, some people continued to work on AI and DL, and some significant advances were made. In 1995, Dana Cortes and Vladimir Vapnik developed the support vector machine (a system for mapping and recognizing similar data). LSTM (long short-term memory) for recurrent neural networks was developed in 1997, by Sepp Hochreiter and Juergen Schmidhuber.
	The next significant evolutionary step for deep learning took place in 1999, when computers started becoming faster at processing data and GPU (graphics processing units) were developed. Faster processing, with GPUs processing pictures, increased computational speeds by 1000 times over a 10 year span. During this time, neural networks began to compete with support vector machines. While a neural network could be slow compared to a support vector machine, neural networks offered better results using the same data. Neural networks also have the advantage of continuing to improve as more training data is added.

- 2000s and 2010s
	Around the year 2000, The Vanishing Gradient Problem appeared. It was discovered “features” (lessons) formed in lower layers were not being learned by the upper layers, because no learning signal reached these layers. This was not a fundamental problem for all neural networks, just the ones with gradient-based learning methods. The source of the problem turned out to be certain activation functions. A number of activation functions condensed their input, in turn reducing the output range in a somewhat chaotic fashion. This produced large areas of input mapped over an extremely small range. In these areas of input, a large change will be reduced to a small change in the output, resulting in a vanishing gradient. Two solutions used to solve this problem were layer-by-layer pre-training and the development of long short-term memory.
	In 2001, a research report by META Group (now called Gartner) described he challenges and opportunities of data growth as three-dimensional. The report described the increasing volume of data and the increasing speed of data as increasing the range of data sources and types. This was a call to prepare for the onslaught of Big Data, which was just starting.
	In 2009, Fei-Fei Li, an AI professor at Stanford launched ImageNet, assembled a free database of more than 14 million labeled images. The Internet is, and was, full of unlabeled images. Labeled images were needed to train neural nets. Professor Li said, Our vision was that big data would change the way machine learning works. Data drives learning.


===========================================================================================================================================================================


3.a Types of Machine Learning
	- Supervised Machine Learning
		Supervised Machine Learning is defined as when a model gets trained on a “Labelled Dataset”. Labelled datasets have both input and output parameters. In Supervised Learning algorithms learn to map points between inputs and correct outputs. It has both training and validation datasets labelled. 
		There are two main categories of supervised learning that are mentioned below
		Classification
			Classification deals with predicting categorical target variables, which represent discrete classes or labels. For instance, classifying emails as spam or not spam, or predicting whether a patient has a high risk of heart disease. Classification algorithms learn to map the input features to one of the predefined classes.
			Here are some classification algorithms:
				Logistic Regression
				Support Vector Machine
				Random Forest
				Decision Tree
				K-Nearest Neighbors (KNN)
				Naive Bayes
		Regression
			Regression, on the other hand, deals with predicting continuous target variables, which represent numerical values. For example, predicting the price of a house based on its size, location, and amenities, or forecasting the sales of a product. Regression algorithms learn to map the input features to a continuous numerical value.
			Here are some regression algorithms:
				Linear Regression
				Polynomial Regression
				Ridge Regression
				Lasso Regression
				Decision tree
				Random Forest

	- Unsupervised Machine Learning
		Unsupervised Learning Unsupervised learning is a type of machine learning technique in which an algorithm discovers patterns and relationships using unlabeled data. Unlike supervised learning, unsupervised learning doesn’t involve providing the algorithm with labeled target outputs. The primary goal of Unsupervised learning is often to discover hidden patterns, similarities, or clusters within the data, which can then be used for various purposes, such as data exploration, visualization, dimensionality reduction, and more.
		There are two main categories of unsupervised learning that are mentioned below
			Clustering
				Clustering is the process of grouping data points into clusters based on their similarity. This technique is useful for identifying patterns and relationships in data without the need for labeled examples.
				Here are some clustering algorithms:
					K-Means Clustering algorithm
					Mean-shift algorithm
					DBSCAN Algorithm
					Principal Component Analysis
					Independent Component Analysis

			Association
				Association rule learning is a technique for discovering relationships between items in a dataset. It identifies rules that indicate the presence of one item implies the presence of another item with a specific probability.
				Here are some association rule learning algorithms:
					Apriori Algorithm
					Eclat
					FP-growth Algorithm


	- Semi-Supervised learning 
			Semi-Supervised learning is a machine learning algorithm that works between the supervised and unsupervised learning so it uses both labelled and unlabelled data. It’s particularly useful when obtaining labeled data is costly, time-consuming, or resource-intensive. This approach is useful when the dataset is expensive and time-consuming. Semi-supervised learning is chosen when labeled data requires skills and relevant resources in order to train or learn from it.
			We use these techniques when we are dealing with data that is a little bit labeled and the rest large portion of it is unlabeled. We can use the unsupervised techniques to predict labels and then feed these labels to supervised techniques. This technique is mostly applicable in the case of image data sets where usually all images are not labeled. 

	- Reinforcement Learning
		Reinforcement machine learning algorithm is a learning method that interacts with the environment by producing actions and discovering errors. Trial, error, and delay are the most relevant characteristics of reinforcement learning. In this technique, the model keeps on increasing its performance using Reward Feedback to learn the behavior or pattern. These algorithms are specific to a particular problem e.g. Google Self Driving car, AlphaGo where a bot competes with humans and even itself to get better and better performers in Go Game. Each time we feed in data, they learn and add the data to their knowledge which is training data. So, the more it learns the better it gets trained and hence experienced. 

------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X--------------

3.b Types of Deep Learning
	- Feedforward Neural Networks (FNN): Also known as Multi-Layer Perceptrons (MLP), represent the simplest and foundational form of deep learning. They function with information flowing unidirectionally, exclusively from input layers to output layers, devoid of any feedback loops or connections. FNNs find wide application in tasks such as image classification, text analysis, and regression problems.

	- Convolutional Neural Networks (CNNs) have gained popularity for their exceptional performance in image and video-related assignments. Equipped with specialized convolutional layers, CNNs adeptly detect patterns and features within images, showcasing remarkable prowess in recognizing objects, shapes, and textures.

	- Recurrent Neural Networks (RNNs) cater to sequential data, such as time series or linguistic information. With loops that facilitate the retention of information over time, RNNs excel in tasks like speech recognition, language modeling, and translation.

	- Long-term Memory Networks (LSTM) are an intriguing variation of RNNs, specifically designed to address the leakage gradient problem. As a result, LSTMs demonstrate superior ability in capturing long-term dependencies in sequential data, presenting valuable solutions in various applications

	- Generative Adversarial Networks (GANs) stand out as a compelling duo of neural networks, consisting of a generative and a discriminative network, both trained together in a competitive process. GANs excel in generating realistic synthetic data, including images, audio, and text, making them a powerful tool in creative endeavors.

	- Autoencoders, on the other hand, function as unsupervised deep learning models, specifically designed for dimensionality reduction and feature learning. Comprising an encoder and a decoder, autoencoders aim to reconstruct input data, thereby facilitating effective feature extraction.

	- Transformer networks have emerged as a transformative neural network architecture. These networks utilize self-attentive mechanisms, enabling parallel processing of input data, which significantly enhances their ability to handle long-range dependencies.

	- Capsule networks present a novel architectural approach to boost feature learning efficiency. Rather than relying on individual neurons, capsule networks represent visual concepts as capsules, promising to push the boundaries of feature representation in visual tasks.

------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X----------------------X--------------

3.c Difference Between ML and DL


===========================================================================================================================================================================


4. Applications of Machine Learning and Deep Learning
	- Image classification: Identify objects, faces, and other features in images.
	- Natural language processing: Extract information from text, such as sentiment, entities, and relationships.
	- Speech recognition: Convert spoken language into text.
	- Recommendation systems: Make personalized recommendations to users.
	- Predictive analytics: Predict outcomes, such as sales, customer churn, and stock prices.
	- Medical diagnosis: Detect diseases and other medical conditions.
	- Fraud detection: Identify fraudulent transactions.
	- Autonomous vehicles: Recognize and respond to objects in the environment.
	- Email spam detection: Classify emails as spam or not spam.
	- Quality control in manufacturing: Inspect products for defects.
	- Credit scoring: Assess the risk of a borrower defaulting on a loan.
	- Gaming: Recognize characters, analyze player behavior, and create NPCs.
	- Customer support: Automate customer support tasks.
	- Weather forecasting: Make predictions for temperature, precipitation, and other meteorological parameters.
	- Sports analytics: Analyze player performance, make game predictions, and optimize strategies.
	- Clustering: Group similar data points into clusters.
	- Anomaly detection: Identify outliers or anomalies in data.
	- Dimensionality reduction: Reduce the dimensionality of data while preserving its essential information.
	- Recommendation systems: Suggest products, movies, or content to users based on their historical behavior or preferences.
	- Topic modeling: Discover latent topics within a collection of documents.
	- Density estimation: Estimate the probability density function of data.
	- Image and video compression: Reduce the amount of storage required for multimedia content.
	- Data preprocessing: Help with data preprocessing tasks such as data cleaning, imputation of missing values, and data scaling.
	- Market basket analysis: Discover associations between products.
	- Genomic data analysis: Identify patterns or group genes with similar expression profiles.
	- Image segmentation: Segment images into meaningful regions.
	- Community detection in social networks: Identify communities or groups of individuals with similar interests or connections.
	- Customer behavior analysis: Uncover patterns and insights for better marketing and product recommendations.
	- Content recommendation: Classify and tag content to make it easier to recommend similar items to users.
	- Exploratory data analysis (EDA): Explore data and gain insights before defining specific tasks.
	- Computer Vision: Applied in diverse areas like object recognition, object detection, image classification, and image segmentation. Its applications range from enabling autonomous vehicles to enhancing security and surveillance systems, as well as supporting medical image analysis.
	- Natural Language Processing (NLP): Empowers speech recognition, machine translation, text generation, sentiment analysis, and the creation of chatbots, fostering more natural and intuitive interactions with computers and devices.
	- Healthcare and Medicine: Plays a pivotal role in medical diagnostics, utilizing medical image analysis and disease detection. Additionally, it finds applications in drug research and clinical data analysis, advancing healthcare practices.
	- Finance and Commerce: Contributes significantly to risk analysis, stock price prediction, fraud detection, and the optimization of trading strategies, driving innovation in the financial industry.
	- Games and Entertainment: Demonstrates remarkable capabilities in challenging board games such as chess and Go, surpassing human players. Additionally, it fosters creativity by generating art and generative music.
	- Robotics: Employs deep learning algorithms in the control and decision-making of autonomous robots, enabling them to navigate unfamiliar environments and undertake complex tasks.
	- Search and Recommendation: Enhances search engines and recommendation systems, optimizing the accuracy of search results and delivering personalized suggestions across various domains.

===========================================================================================================================================================================


5 Libraries used for Machine Learning and Deep Learning
	- Numpy
		NumPy is a very popular python library for large multi-dimensional array and matrix processing, with the help of a large collection of high-level mathematical functions. It is very useful for fundamental scientific computations in Machine Learning. It is particularly useful for linear algebra, Fourier transform, and random number capabilities. High-end libraries like TensorFlow uses NumPy internally for manipulation of Tensors. 

	- Pandas 
		Pandas is a popular Python library for data analysis. It is not directly related to Machine Learning. As we know that the dataset must be prepared before training.	
		In this case, Pandas comes handy as it was developed specifically for data extraction and preparation.
		It provides high-level data structures and wide variety tools for data analysis. It provides many inbuilt methods for grouping, combining and filtering data.
	
	- Matplotlib
		Matplotlib is a very popular Python library for data visualization. Like Pandas, it is not directly related to Machine Learning. It particularly comes in handy when a programmer wants to visualize the patterns in the data. It is a 2D plotting library used for creating 2D graphs and plots.
		A module named pyplot makes it easy for programmers for plotting as it provides features to control line styles, font properties, formatting axes, etc.
		It provides various kinds of graphs and plots for data visualization, viz., histogram, error charts, bar chats, etc

	- SciPy
		SciPy is a very popular library among Machine Learning enthusiasts as it contains different modules for optimization, linear algebra, integration and statistics. There is a difference between the SciPy library and the SciPy stack. The SciPy is one of the core packages that make up the SciPy stack. SciPy is also very useful for image manipulation.  

	- Scikit-Learn
		Scikit-learn is one of the most popular ML libraries for classical ML algorithms. It is built on top of two basic Python libraries, viz., NumPy and SciPy. Scikit-learn supports most of the supervised and unsupervised learning algorithms. Scikit-learn can also be used for data-mining and data-analysis, which makes it a great tool who is starting out with ML. 

	- Theano
		We all know that Machine Learning is basically mathematics and statistics. Theano is a popular python library that is used to define, evaluate and optimize mathematical expressions involving multi-dimensional arrays in an efficient manner.
		It is achieved by optimizing the utilization of CPU and GPU. It is extensively used for unit-testing and self-verification to detect and diagnose different types of errors. Theano is a very powerful library that has been used in large-scale computationally intensive scientific projects for a long time but is simple and approachable enough to be used by individuals for their own projects. 

	- TensorFlow
		TensorFlow is a very popular open-source library for high performance numerical computation developed by the Google Brain team in Google. As the name suggests, Tensorflow is a framework that involves defining and running computations involving tensors. It can train and run deep neural networks that can be used to develop several AI applications. TensorFlow is widely used in the field of deep learning research and application.

	- Keras
		v vKeras is a very popular Python Libaries for Machine Learning . It is a high-level neural networks API capable of running on top of TensorFlow, CNTK, or Theano. It can run seamlessly on both CPU and GPU. Keras makes it really for ML beginners to build and design a Neural Network. One of the best thing about Keras is that it allows for easy and fast prototyping.

	- PyTorch
		PyTorch is a popular open-source Python Library for Machine Learning based on Torch, which is an open-source Machine Learning library that is implemented in C with a wrapper in Lua. It has an extensive choice of tools and libraries that support Computer Vision, Natural Language Processing(NLP), and many more ML programs. It allows developers to perform computations on Tensors with GPU acceleration and also helps in creating computational graphs. 


===========================================================================================================================================================================

5. Math behind libraries

	





























